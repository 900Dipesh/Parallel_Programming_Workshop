{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management with OpenACC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the lab is intended for C/C++ programmers. The Fortran version of this lab is available [here](../Fortran/README.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will receive a warning five minutes before the lab instance shuts down. Remember to save your work! If you are about to run out of time, please see the [Post-Lab](#Post-Lab-Summary) section for saving this lab to view offline later.\n",
    "\n",
    "Don't forget to check out additional [OpenACC Resources](https://www.openacc.org/resources) and join our [OpenACC Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's execute the cell below to display information about the GPUs running on the server. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 19 07:43:45 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   47C    P0    26W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Our goal for this lab is to use the OpenACC Data Directives to properly manage our data.\n",
    "  \n",
    "  \n",
    "  \n",
    "![development_cycle.png](../images/development_cycle.png)\n",
    "\n",
    "This is the OpenACC 3-Step development cycle.\n",
    "\n",
    "**Analyze** your code, and predict where potential parallelism can be uncovered. Use profiler to help understand what is happening in the code, and where parallelism may exist.\n",
    "\n",
    "**Parallelize** your code, starting with the most time consuming parts. Focus on maintaining correct results from your program.\n",
    "\n",
    "**Optimize** your code, focusing on maximizing performance. Performance may not increase all-at-once during early parallelization.\n",
    "\n",
    "We are currently tackling the **parallelize** step. We will include the OpenACC data directive to properly manage data within our parallelized code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run the Code (With Managed Memory)\n",
    "\n",
    "In the previous lab, we ran our code with CUDA Managed Memory, and achieved a considerable performance boost. However, managed memory is not compatible with all GPUs, and it performs it may performs worsemmer defined, proper memory management. Run the following script, and note the time the program takes to run. We are expecting that our own implementation which we will develop later in this lab will run a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacobi.c:\n",
      "laplace2d.c:\n",
      "calcNext:\n",
      "     36, Generating copy(A[:n*m]) [if not already present]\n",
      "         Generating Tesla code\n",
      "         38, #pragma acc loop gang /* blockIdx.x */\n",
      "             Generating implicit reduction(max:error)\n",
      "         41, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "     36, Generating implicit copy(error) [if not already present]\n",
      "         Generating copy(Anew[:n*m]) [if not already present]\n",
      "     41, Loop is parallelizable\n",
      "swap:\n",
      "     52, Generating copy(A[:n*m],Anew[:n*m]) [if not already present]\n",
      "         Generating Tesla code\n",
      "         54, #pragma acc loop gang /* blockIdx.x */\n",
      "         57, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "     57, Loop is parallelizable\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 1.161982 s\n"
     ]
    }
   ],
   "source": [
    "!nvc -fast -ta=tesla:managed -Minfo=accel -o laplace_managed jacobi.c laplace2d.c && ./laplace_managed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Analyze the Code\n",
    "\n",
    "If you would like a refresher on the code files that we are working on, you may view both of them using the two links below.\n",
    "\n",
    "[jacobi.c](jacobi.c)  \n",
    "[laplace2d.c](laplace2d.c)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Profile the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: OpenACC, cuDNN and cuBLAS rely on CUDA. CUDA tracing has been automatically enabled.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 1.254324 s\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-9e41-bff6-e732-d1bd.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-9e41-bff6-e732-d1bd.qdrep\"\n",
      "Exporting 71111 events: [=================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-9e41-bff6-e732-d1bd.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name        \n",
      " -------  ---------------  ---------  ----------  --------  --------  --------------------\n",
      "    90.5       1094611316       4000    273652.8         0  65812179  cuStreamSynchronize \n",
      "     2.8         33798463       3000     11266.2         0     65317  cuLaunchKernel      \n",
      "     2.6         31190135          1  31190135.0  31190135  31190135  cuMemHostAlloc      \n",
      "     1.7         20348985          1  20348985.0  20348985  20348985  cuMemAllocManaged   \n",
      "     1.0         11702421       1000     11702.4        57     57963  cuMemcpyDtoHAsync_v2\n",
      "     0.7          8706854       1000      8706.9         0     55116  cuMemsetD32Async    \n",
      "     0.3          3448425       1000      3448.4         0     52930  cuEventSynchronize  \n",
      "     0.3          3278834       1000      3278.8         0     52395  cuEventRecord       \n",
      "     0.1           970770          1    970770.0    970770    970770  cuMemAllocHost_v2   \n",
      "     0.0           484666          3    161555.3      8123    449376  cuMemAlloc_v2       \n",
      "     0.0           355979          1    355979.0    355979    355979  cuModuleLoadDataEx  \n",
      "     0.0            31163          1     31163.0     31163     31163  cuStreamCreate      \n",
      "     0.0             4838          2      2419.0      1617      3221  cuEventCreate       \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances  Average   Minimum  Maximum           Name        \n",
      " -------  ---------------  ---------  --------  -------  --------  --------------------\n",
      "    58.5        637768300       1000  637768.3   556760  65831326  calcNext_36_gpu     \n",
      "    40.8        444568993       1000  444569.0   433753    457049  swap_52_gpu         \n",
      "     0.7          7562770       1000    7562.8     7039      9408  calcNext_36_gpu__red\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    93.7         44040757        7709   5712.9     2846    85342  [CUDA Unified Memory memcpy HtoD]\n",
      "     3.3          1555724        1000   1555.7     1503     3744  [CUDA memcpy DtoH]               \n",
      "     3.0          1427783        1000   1427.8     1376     4032  [CUDA memset]                    \n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum              Operation            \n",
      " ----------  ----------  -------  -------  -------  ---------------------------------\n",
      "      7.813        1000    0.008    0.008    0.008  [CUDA memcpy DtoH]               \n",
      "      7.813        1000    0.008    0.008    0.008  [CUDA memset]                    \n",
      " 262720.000        7709   34.080    4.000  888.000  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "Report file moved to \"/home/openacc/labs/module5/English/C/laplace_managed.qdrep\"\n",
      "Report file moved to \"/home/openacc/labs/module5/English/C/laplace_managed.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile -t openacc --stats=true --force-overwrite true -o laplace_managed ./laplace_managed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the profiler's report. Once the profiling run has completed, download and save the report file by holding down <mark>Shift</mark> and <mark>Right-Clicking</mark> [Here](laplace_managed.qdrep) (choose *save link as*), and open it via the GUI. To view the profiler report locally, please see the section on [How to view the report](../../../module2/English/C/README.ipynb#viewreport)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## OpenACC Structured Data Directive\n",
    "\n",
    "The OpenACC data directives allow the programmer to explicitly manage the data on the device (in our case, the GPU). Specifically, the structured data directive will mark a static region of our code as a **data region**.\n",
    "\n",
    "```cpp\n",
    "< Initialize data on host (CPU) >\n",
    "\n",
    "#pragma acc data < data clauses >\n",
    "{\n",
    "\n",
    "    < Code >\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "Device memory allocation happens at the beginning of the region, and device memory deallocation happens at the end of the region. Additionally, any data movement from the host to the device (CPU to GPU) happens at the beginning of the region, and any data movement from the device to the host (GPU to CPU) happens at the end of the region. Memory allocation/deallocation and data movement is defined by which clauses the programmer includes. This is a list of the most important data clauses that we can use:\n",
    "\n",
    "**copy** : `copy( A[0:N] )` : Allocates memory on device and copies data from host to device when entering region and copies data back to the host when exiting region  \n",
    "**copyin** : `copyin( A[0:N] )` : Allocates memory on device and copies data from host to device when entering region  \n",
    "**copyout** : `copyout( A[0:N] )` : Allocates memory on device and copies data to the host when exiting region  \n",
    "**create** : `create( A[0:N] )` : Allocates memory on device but does not copy  \n",
    "**present** : `present( A )` : Data is already present on device from another containing data region  \n",
    "\n",
    "All of these data clauses (except for present) will allocate device memory at the beginning of the data region, and deallocate device memory at the end of the data region. And with the exception of create, they will also transfer some amount of data between the host and device.\n",
    "\n",
    "You may also use them to operate on multiple arrays at once, by including those arrays as a comma separated list.\n",
    "\n",
    "```cpp\n",
    "#pragma acc data copy( A[0:N], B[0:M], C[0:Q] )\n",
    "```\n",
    "\n",
    "You may also use more than one data clause at a time.\n",
    "\n",
    "```cpp\n",
    "#pragma acc data create( A[0:N] ) copyin( B[0:M] ) copyout( C[0:Q] )\n",
    "```\n",
    "\n",
    "These clauses can also be used directly with a parallel or kernels directive, because every parallel and kernels directive is surrounded by an **implied data region**.\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels create(A[0:N]) copyin(B[0:M]) present(C[0:Q])\n",
    "{\n",
    "    < Code that uses A, B, and C >\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encompassing Multiple Compute Regions\n",
    "\n",
    "A single data region can contain any number of parallel/kernels regions. Take the following example:\n",
    "\n",
    "```cpp\n",
    "#pragma acc data copyin(A[0:N], B[0:N]) create(C[0:N])\n",
    "{\n",
    "\n",
    "    #pragma acc parallel loop\n",
    "    for( int i = 0; i < N; i++ )\n",
    "    {\n",
    "        C[i] = A[i] + B[i];\n",
    "    }\n",
    "    \n",
    "    #pragma acc parallel loop\n",
    "    for( int i = 0; i < N; i++ )\n",
    "    {\n",
    "        A[i] = C[i] + B[i];\n",
    "    }\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "You may also encompass function calls within the data region:\n",
    "\n",
    "```cpp\n",
    "void copy(int *A, int *B, int N)\n",
    "{\n",
    "    #pragma acc parallel loop copyout(A[0:N]) copyin(B[0:N])\n",
    "    for( int i = 0; i < N; i++ )\n",
    "    {\n",
    "        A[i] = B[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "...\n",
    "\n",
    "#pragma acc data copyout(A[0:N],B[0:N]) copyin(C[0:N])\n",
    "{\n",
    "    copy(A, C, N);\n",
    "    \n",
    "    copy(A, B, N);\n",
    "}\n",
    "```\n",
    "\n",
    "But wouldn't this code now result in my arrays being copied twice, once by the data region and then again by the parallel loop inside the function calls? In fact, the OpenACC runtime is smart enough to handle exactly this case. Data will be copied in only the first time its encountered in a data clause and out only the last time its encountered in a data clause. This allows you to create fully-working directives within your functions and then later *\"hoist\"* the data movement to a higher level without changing your code at all. This is part of incrementally accelerating your code to avoid incorrect results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Shaping\n",
    "\n",
    "The *array shape* defines a portion of an array. Take the following example:\n",
    "\n",
    "```cpp\n",
    "int *A = (int*) malloc(N * sizeof(int));\n",
    "\n",
    "#pragma acc data create( A[0:N] )\n",
    "```\n",
    "\n",
    "The array shape is defined as [0:N], this means that the GPU copy will start at index 0, and be of size N. Array shape is of the format **Array[starting_index:size]**. Let's look at an example where we only want a portion of the array.\n",
    "\n",
    "```cpp\n",
    "int *A = (int*) malloc(N * sizeof(int));\n",
    "\n",
    "#pragma acc data create( A[0:N/2] )\n",
    "```\n",
    "\n",
    "In this example, the GPU copy will start at index 0, but will only be half the size of the CPU copy.\n",
    "\n",
    "The shape of multi-dimensional arrays can be defined as follows:\n",
    "\n",
    "```cpp\n",
    "#pragma acc data create( A[0:N][0:M] )\n",
    "```\n",
    "\n",
    "If you do not include a starting index, then 0 is assumed. For example:\n",
    "\n",
    "```cpp\n",
    "#pragma acc data create( A[0:N] )\n",
    "```\n",
    "\n",
    "is equivalent to\n",
    "\n",
    "```cpp\n",
    "#pragma acc data create( A[:N] )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host or Device Memory?\n",
    "\n",
    "Here are two loops:\n",
    "\n",
    "```cpp\n",
    "int *A = (int*) malloc(N * sizeof(int));\n",
    "\n",
    "for (int i = 0; i < N; i++ )\n",
    "{\n",
    "    A[i] = 0;\n",
    "}\n",
    "\n",
    "#pragma acc parallel loop\n",
    "for( int i = 0; i < N; i++ )\n",
    "{\n",
    "    A[i] = 1;\n",
    "}\n",
    "```\n",
    "\n",
    "The first loop is not contained within an OpenACC compute region (a compute region is marked by either the parallel or kernels directive). Thus, `A[i]` will access host (CPU) memory.\n",
    "\n",
    "The second loop is preceeded by the *parallel directive*, meaning that it is contained within an OpenACC compute region. `A[i]` in the second loop will access device (GPU) memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Structured Data Directive to our Code\n",
    "\n",
    "Use the following links to edit our laplace code. Add a structured data directive to properly handle the arrays `A` and `Anew`. \n",
    "\n",
    "[jacobi.c](jacobi.c)   \n",
    "[laplace2d.c](laplace2d.c)  \n",
    "\n",
    "Then, run the following script to check your solution. You code should run just as good as (or slightly better) than our managed memory code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacobi.c:\n",
      "main:\n",
      "     59, Generating create(Anew[:m*n]) [if not already present]\n",
      "         Generating copyin(A[:m*n]) [if not already present]\n",
      "laplace2d.c:\n",
      "calcNext:\n",
      "     36, Generating copy(A[:n*m]) [if not already present]\n",
      "         Generating Tesla code\n",
      "         40, #pragma acc loop gang /* blockIdx.x */\n",
      "             Generating implicit reduction(max:error)\n",
      "         43, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "     36, Generating implicit copy(error) [if not already present]\n",
      "         Generating copy(Anew[:n*m]) [if not already present]\n",
      "     43, Loop is parallelizable\n",
      "swap:\n",
      "     54, Generating copy(A[:n*m],Anew[:n*m]) [if not already present]\n",
      "         Generating Tesla code\n",
      "         57, #pragma acc loop gang /* blockIdx.x */\n",
      "         60, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "     60, Loop is parallelizable\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 1.234527 s\n"
     ]
    }
   ],
   "source": [
    "!nvc -fast -ta=tesla -Minfo=accel -o laplace_structured jacobi.c laplace2d.c && ./laplace_structured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are feeling stuck, or would like to check your answer, you can view the correct answer with the following link.\n",
    "\n",
    "[jacobi.c](solutions/advanced_data/structured/jacobi.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Profile the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: OpenACC, cuDNN and cuBLAS rely on CUDA. CUDA tracing has been automatically enabled.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 1.506861 s\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-a329-176b-b327-ccb3.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-a329-176b-b327-ccb3.qdrep\"\n",
      "Exporting 60205 events: [=================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-a329-176b-b327-ccb3.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name        \n",
      " -------  ---------------  ---------  ----------  --------  --------  --------------------\n",
      "    91.8       1045022147       4001    261190.2         0    641280  cuStreamSynchronize \n",
      "     2.8         31496094          1  31496094.0  31496094  31496094  cuMemHostAlloc      \n",
      "     2.6         29257496       3000      9752.5         0     92887  cuLaunchKernel      \n",
      "     1.1         12991705       1000     12991.7         0     88001  cuMemcpyDtoHAsync_v2\n",
      "     0.8          9610337       1000      9610.3      2427     55513  cuMemsetD32Async    \n",
      "     0.3          3426890       1007      3403.1         0     47334  cuEventRecord       \n",
      "     0.3          3156065       1006      3137.2         0     18194  cuEventSynchronize  \n",
      "     0.2          2286105          5    457221.0      9027   1527127  cuMemAlloc_v2       \n",
      "     0.1          1161364          1   1161364.0   1161364   1161364  cuMemAllocHost_v2   \n",
      "     0.0           331364          1    331364.0    331364    331364  cuModuleLoadDataEx  \n",
      "     0.0           104648          8     13081.0         0     30577  cuMemcpyHtoDAsync_v2\n",
      "     0.0            29015          1     29015.0     29015     29015  cuStreamCreate      \n",
      "     0.0             8502          3      2834.0      1188      3944  cuEventCreate       \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances  Average   Minimum  Maximum          Name        \n",
      " -------  ---------------  ---------  --------  -------  -------  --------------------\n",
      "    55.5        574827249       1000  574827.2   555773   592157  calcNext_36_gpu     \n",
      "    43.7        452554701       1000  452554.7   442910   467134  swap_54_gpu         \n",
      "     0.7          7487486       1000    7487.5     7008     8896  calcNext_36_gpu__red\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations   Average   Minimum  Maximum      Operation     \n",
      " -------  ---------------  ----------  ---------  -------  -------  ------------------\n",
      "    80.2         12091880           8  1511485.0  1508953  1520185  [CUDA memcpy HtoD]\n",
      "    10.3          1557427        1000     1557.4     1503     3072  [CUDA memcpy DtoH]\n",
      "     9.5          1428149        1000     1428.1     1376     2432  [CUDA memset]     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations   Average    Minimum    Maximum       Operation     \n",
      " ----------  ----------  ---------  ---------  ---------  ------------------\n",
      "      7.813        1000      0.008      0.008      0.008  [CUDA memcpy DtoH]\n",
      "      7.813        1000      0.008      0.008      0.008  [CUDA memset]     \n",
      " 131072.000           8  16384.000  16384.000  16384.000  [CUDA memcpy HtoD]\n",
      "\n",
      "Report file moved to \"/home/openacc/labs/module5/English/C/laplace_structured.qdrep\"\n",
      "Report file moved to \"/home/openacc/labs/module5/English/C/laplace_structured.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile -t openacc --stats=true --force-overwrite true -o laplace_structured ./laplace_structured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the profiler's report. Once the profiling run has completed, download and save the report file by holding down <mark>Shift</mark> and <mark>Right-Clicking</mark> [Here](laplace_structured.qdrep) (choose *save link as*), and open it via the GUI. To view the profiler report locally, please see the section on [How to view the report](../../../module2/English/C/README.ipynb#viewreport).\n",
    "\n",
    "Take a moment to explore the profiler, and when you're ready, let's zoom in on the very beginning of our profile.\n",
    "\n",
    "![structured.PNG](../images/structured.png)\n",
    "\n",
    "We can see that we have uninterupted computation, and all of our data movement happens at the beginning of the program. This is ideal, because we are avoiding data transers in the middle of our computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## OpenACC Unstructured Data Directives\n",
    "\n",
    "There are two unstructured data directives:\n",
    "\n",
    "**enter data**: Handles device memory allocation, and copies from the Host to the Device. The two clauses that you may use with `enter data` are `create` for device memory allocation, and `copyin` for allocation, and memory copy.\n",
    "\n",
    "**exit data**: Handles device memory deallocation, and copies from the Device to the Host. The two clauses that you may use with `exit data` are `delete` for device memory deallocation, and `copyout` for deallocation, and memory copy.\n",
    "\n",
    "The unstructured data directives do not mark a \"data region\", because you are able to have multiple `enter data` and `exit data` directives in your code. It is better to think of them purely as memory allocation and deallocation.\n",
    "\n",
    "The largest advantage of using unstructured data directives is their ability to branch across multiple functions. You may allocate your data in one function, and deallocate it in another. We can look at a simple example of that:\n",
    "\n",
    "```cpp\n",
    "int* allocate(int size)\n",
    "{\n",
    "    int *ptr = (int*) malloc(size * sizeof(int));\n",
    "    #pragma acc enter data create(ptr[0:size])\n",
    "    return ptr;\n",
    "}\n",
    "\n",
    "void deallocate(int *ptr)\n",
    "{\n",
    "    #pragma acc exit data delete(ptr)\n",
    "    free(ptr);\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    int *ptr = allocate(100);\n",
    "    \n",
    "    #pragma acc parallel loop\n",
    "    for( int i = 0; i < 100; i++ )\n",
    "    {\n",
    "        ptr[i] = 0;\n",
    "    }\n",
    "    \n",
    "    deallocate(ptr);\n",
    "}\n",
    "```\n",
    "\n",
    "Just like in the above code sample, you must first allocate the CPU copy of the array **before** you can allocate the GPU copy. Also, you must deallocate the GPU of the array **before** you deallocate the CPU copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Unstructured Data Directives to our Code\n",
    "\n",
    "We are going to edit our code to use unstructured data directives to handle memory management. First, run the following script to reset your code to how it was before adding the structured data directive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset Finished\n"
     ]
    }
   ],
   "source": [
    "!cp ./solutions/basic_data/jacobi.c ./jacobi.c && cp ./solutions/basic_data/laplace2d.c ./laplace2d.c && echo \"Reset Finished\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now edit the code to use unstructured data directives. To fully utilize the unstructured data directives, try to get the code working by only altering the **laplace2d.c** code.\n",
    "\n",
    "[jacobi.c](jacobi.c)   \n",
    "[laplace2d.c](laplace2d.c)  \n",
    "\n",
    "Run the following script to check your solution. Your code should run as fast as our structured implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacobi.c:\n",
      "laplace2d.c:\n",
      "initialize:\n",
      "     33, Generating enter data copyin(Anew[:m*n],A[:m*n])\n",
      "calcNext:\n",
      "     37, Generating copy(A[:n*m]) [if not already present]\n",
      "         Generating Tesla code\n",
      "         39, #pragma acc loop gang /* blockIdx.x */\n",
      "             Generating implicit reduction(max:error)\n",
      "         42, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "     37, Generating implicit copy(error) [if not already present]\n",
      "         Generating copy(Anew[:n*m]) [if not already present]\n",
      "     42, Loop is parallelizable\n",
      "swap:\n",
      "     53, Generating copy(A[:n*m],Anew[:n*m]) [if not already present]\n",
      "         Generating Tesla code\n",
      "         55, #pragma acc loop gang /* blockIdx.x */\n",
      "         58, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "     58, Loop is parallelizable\n",
      "deallocate:\n",
      "     68, Generating exit data delete(Anew,A)\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 1.085706 s\n"
     ]
    }
   ],
   "source": [
    "!nvc -fast -ta=tesla -Minfo=accel -o laplace_unstructured jacobi.c laplace2d.c && ./laplace_unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are feeling stuck, or would like to check your answer, you can view the correct answer with the following link.\n",
    "\n",
    "[laplace2d.c](solutions/advanced_data/unstructured/laplace2d.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Profile the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: OpenACC, cuDNN and cuBLAS rely on CUDA. CUDA tracing has been automatically enabled.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 1.163867 s\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-55c3-eec6-581c-0b3d.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-55c3-eec6-581c-0b3d.qdrep\"\n",
      "Exporting 60244 events: [=================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-55c3-eec6-581c-0b3d.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name        \n",
      " -------  ---------------  ---------  ----------  --------  --------  --------------------\n",
      "    92.0       1044215885       4001    260988.7         0   1502823  cuStreamSynchronize \n",
      "     2.7         30831900          1  30831900.0  30831900  30831900  cuMemHostAlloc      \n",
      "     2.6         29944789       3000      9981.6         0     64396  cuLaunchKernel      \n",
      "     1.0         11365623       1000     11365.6      3567     61031  cuMemcpyDtoHAsync_v2\n",
      "     0.8          8671188       1000      8671.2         0     57532  cuMemsetD32Async    \n",
      "     0.3          3351682       1014      3305.4         0     18426  cuEventSynchronize  \n",
      "     0.3          3218351       1015      3170.8         0     27314  cuEventRecord       \n",
      "     0.2          1964917          5    392983.4      9225   1105735  cuMemAlloc_v2       \n",
      "     0.1          1171490          1   1171490.0   1171490   1171490  cuMemAllocHost_v2   \n",
      "     0.0           374853          1    374853.0    374853    374853  cuModuleLoadDataEx  \n",
      "     0.0           202140         16     12633.8      6417     44184  cuMemcpyHtoDAsync_v2\n",
      "     0.0            21504          1     21504.0     21504     21504  cuStreamCreate      \n",
      "     0.0             8047          3      2682.3      1734      3454  cuEventCreate       \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances  Average   Minimum  Maximum          Name        \n",
      " -------  ---------------  ---------  --------  -------  -------  --------------------\n",
      "    55.6        575340157       1000  575340.2   560055   591703  calcNext_37_gpu     \n",
      "    43.7        452362586       1000  452362.6   441913   464121  swap_53_gpu         \n",
      "     0.7          7493603       1000    7493.6     7008     8672  calcNext_37_gpu__red\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations   Average   Minimum  Maximum      Operation     \n",
      " -------  ---------------  ----------  ---------  -------  -------  ------------------\n",
      "    89.6         25592002          16  1599500.1  1508456  2036927  [CUDA memcpy HtoD]\n",
      "     5.4          1556619        1000     1556.6     1503     4160  [CUDA memcpy DtoH]\n",
      "     5.0          1428263        1000     1428.3     1375     2016  [CUDA memset]     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations   Average    Minimum    Maximum       Operation     \n",
      " ----------  ----------  ---------  ---------  ---------  ------------------\n",
      "      7.813        1000      0.008      0.008      0.008  [CUDA memcpy DtoH]\n",
      "      7.813        1000      0.008      0.008      0.008  [CUDA memset]     \n",
      " 262144.000          16  16384.000  16384.000  16384.000  [CUDA memcpy HtoD]\n",
      "\n",
      "Report file moved to \"/home/openacc/labs/module5/English/C/laplace_unstructured.qdrep\"\n",
      "Report file moved to \"/home/openacc/labs/module5/English/C/laplace_unstructured.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile -t openacc --stats=true --force-overwrite true -o laplace_unstructured ./laplace_unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the profiler's report. Once the profiling run has completed, download and save the report file by holding down <mark>Shift</mark> and <mark>Right-Clicking</mark> [Here](laplace_unstructured.qdrep) (choose *save link as*), and open it via the GUI. To view the profiler report locally, please see the section on [How to view the report](../../../module2/English/C/README.ipynb#viewreport).\n",
    "\n",
    "Take a moment to explore the profiler, and when you're ready, let's zoom in on the very beginning of our profile.\n",
    "\n",
    "![unstructured.PNG](../images/unstructured.png)\n",
    "\n",
    "We can see that we have uninterupted computation, and all of our data movement happens at the beginning of the program. This is ideal, because we are avoiding data transers in the middle of our computation. If you also profiled the structured version of the code, you will notice that the profiles are nearly identical. This isn't surprising, since the structured and unstructured approach work very similarly at the hardware level. However, structured data regions may be easier in simple codes, whereas some codes might flow better when using an unstructured approach. It is up to the programmer to determine via analysis and profiling, which to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## OpenACC Update Directive\n",
    "\n",
    "When we use the data directives, there exist two places where the programmer can transfer data between the host and the device. For the structured data directive we have the opportunity to transfer data at the beginning and at the end of the region. For the unstructured data directives, we can transfer data when we use the enter data and exit data directives.\n",
    "\n",
    "However, there may be times in your program where you need to transfer data in the middle of a data region, or between an enter data and an exit data. In order to transfer data at those times, we can use the `update` directive. The update directive will explicitly transfer data between the host and the device. The `update` directive has two clauses:\n",
    "\n",
    "**self**: The self clause will transfer data from the device to the host (GPU to CPU)  \n",
    "**device**: The device clause will transfer data from the host to the device (CPU to GPU)\n",
    "\n",
    "The syntax would look like:\n",
    "\n",
    "`#pragma acc update self(A[0:N])`\n",
    "\n",
    "`#pragma acc update device(A[0:N])`\n",
    "\n",
    "All of the array shaping rules apply.\n",
    "\n",
    "As an example, let's create a version of our laplace code where we want to print the array **A** after every 100 iterations of our loop. The code will look like this:\n",
    "\n",
    "```cpp\n",
    "#pragma acc data copyin( A[:m*n],Anew[:m*n] )\n",
    "{\n",
    "    while ( error > tol && iter < iter_max )\n",
    "    {\n",
    "        error = calcNext(A, Anew, m, n);\n",
    "        swap(A, Anew, m, n);\n",
    "        \n",
    "        if(iter % 100 == 0)\n",
    "        {\n",
    "            printf(\"%5d, %0.6f\\n\", iter, error);\n",
    "            for( int i = 0; i < n; i++ )\n",
    "            {\n",
    "                for( int j = 0; j < m; j++ )\n",
    "                {\n",
    "                    printf(\"%0.2f \", A[i+j*m]);\n",
    "                }\n",
    "                printf(\"\\n\");\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        iter++;\n",
    "\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Let's run this code (on a very small data set, so that we don't overload the console by printing thousands of numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./update/jacobi.c:\n",
      "main:\n",
      "     60, Generating copyin(Anew[:m*n],A[:m*n]) [if not already present]\n",
      "./update/laplace2d.c:\n",
      "calcNext:\n",
      "     36, Generating copy(A[:n*m]) [if not already present]\n",
      "         Generating Tesla code\n",
      "         38, #pragma acc loop gang /* blockIdx.x */\n",
      "             Generating implicit reduction(max:error)\n",
      "         41, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "     36, Generating implicit copy(error) [if not already present]\n",
      "         Generating copy(Anew[:n*m]) [if not already present]\n",
      "     41, Loop is parallelizable\n",
      "swap:\n",
      "     52, Generating copy(A[:n*m],Anew[:n*m]) [if not already present]\n",
      "         Generating Tesla code\n",
      "         54, #pragma acc loop gang /* blockIdx.x */\n",
      "         57, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "     57, Loop is parallelizable\n",
      "Jacobi relaxation Calculation: 10 x 10 mesh\n",
      "    0, 0.250000\n",
      "1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "  100, 0.000046\n",
      "1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      " total: 0.141817 s\n"
     ]
    }
   ],
   "source": [
    "!nvc -fast -ta=tesla -Minfo=accel -o laplace_no_update ./update/jacobi.c ./update/laplace2d.c && ./laplace_no_update 10 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the array is not changing. This is because the host copy of `A` is not being **updated** between loop iterations. Let's add the update directive, and see how the output changes.\n",
    "\n",
    "```cpp\n",
    "#pragma acc data copyin( A[:m*n],Anew[:m*n] )\n",
    "{\n",
    "    while ( error > tol && iter < iter_max )\n",
    "    {\n",
    "        error = calcNext(A, Anew, m, n);\n",
    "        swap(A, Anew, m, n);\n",
    "        \n",
    "        if(iter % 100 == 0)\n",
    "        {\n",
    "            printf(\"%5d, %0.6f\\n\", iter, error);\n",
    "            \n",
    "            #pragma acc update self(A[0:m*n])\n",
    "            \n",
    "            for( int i = 0; i < n; i++ )\n",
    "            {\n",
    "                for( int j = 0; j < m; j++ )\n",
    "                {\n",
    "                    printf(\"%0.2f \", A[i+j*m]);\n",
    "                }\n",
    "                printf(\"\\n\");\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        iter++;\n",
    "\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./update/solution/jacobi.c:\n",
      "main:\n",
      "     60, Generating copyin(A[:m*n],Anew[:m*n]) [if not already present]\n",
      "     68, Generating update self(A[:m*n])\n",
      "./update/solution/laplace2d.c:\n",
      "calcNext:\n",
      "     36, Generating copy(A[:n*m]) [if not already present]\n",
      "         Generating Tesla code\n",
      "         38, #pragma acc loop gang /* blockIdx.x */\n",
      "             Generating implicit reduction(max:error)\n",
      "         41, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "     36, Generating implicit copy(error) [if not already present]\n",
      "         Generating copy(Anew[:n*m]) [if not already present]\n",
      "     41, Loop is parallelizable\n",
      "swap:\n",
      "     52, Generating copy(A[:n*m],Anew[:n*m]) [if not already present]\n",
      "         Generating Tesla code\n",
      "         54, #pragma acc loop gang /* blockIdx.x */\n",
      "         57, #pragma acc loop vector(128) /* threadIdx.x */\n",
      "     57, Loop is parallelizable\n",
      "Jacobi relaxation Calculation: 10 x 10 mesh\n",
      "    0, 0.250000\n",
      "1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 \n",
      "0.00 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      "  100, 0.000046\n",
      "1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 \n",
      "0.00 0.49 0.67 0.74 0.77 0.77 0.74 0.67 0.49 0.00 \n",
      "0.00 0.28 0.45 0.54 0.58 0.58 0.54 0.45 0.28 0.00 \n",
      "0.00 0.17 0.30 0.38 0.42 0.42 0.38 0.30 0.17 0.00 \n",
      "0.00 0.11 0.20 0.26 0.29 0.29 0.26 0.20 0.11 0.00 \n",
      "0.00 0.07 0.14 0.18 0.20 0.20 0.18 0.14 0.07 0.00 \n",
      "0.00 0.05 0.09 0.12 0.14 0.14 0.12 0.09 0.05 0.00 \n",
      "0.00 0.03 0.05 0.07 0.08 0.08 0.07 0.05 0.03 0.00 \n",
      "0.00 0.01 0.03 0.03 0.04 0.04 0.03 0.03 0.01 0.00 \n",
      "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
      " total: 0.143637 s\n"
     ]
    }
   ],
   "source": [
    "!nvc -fast -ta=tesla -Minfo=accel -o laplace_update ./update/solution/jacobi.c ./update/solution/laplace2d.c && ./laplace_update 10 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Relying on managed memory to handle data management can reduce the effort the programmer needs to parallelize their code, however, not all GPUs work with managed memory, and it is also lower performance than using explicit data management. OpenACC gives the programmer two main ways to handle data management, structured and unstructured data directives. By using these, the programmer is able to minimize the number of data transfers needed in their program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus Task\n",
    "\n",
    "If you would like some additional lessons on using OpenACC, there is an Introduction to OpenACC video series available from the OpenACC YouTube page. The fifth video in the series covers a lot of the content that was covered in this lab.  \n",
    "\n",
    "[Introduction to Parallel Programming with OpenACC - Part 5](https://youtu.be/0zTX7-CPvV8)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, it is recommended you go to your browsers File menu (not the Jupyter notebook file menu) and save the complete web page.  This will ensure the images are copied down as well.\n",
    "\n",
    "You can also execute the following cell block to create a zip-file of the files you've been working on, and download it with the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -f openacc_files.zip\n",
    "zip -r openacc_files.zip *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After** executing the above zip command, you should be able to download and save the zip file by holding down <mark>Shift</mark> and <mark>Right-Clicking</mark> [Here](openacc_files.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Licensing\n",
    "This material is released by NVIDIA Corporation under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
